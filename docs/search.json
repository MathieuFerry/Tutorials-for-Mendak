[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Access tutorial(s) below. For now, only a tutorial in English is available. The tutorial in French is under construction.\nA presentation of some of the equivalent functions to be used in an R console (with a complete tutorial on how to code the statistical analyses) is available here."
  },
  {
    "objectID": "index.html#tutorials-for-mendak",
    "href": "index.html#tutorials-for-mendak",
    "title": "Tutorials",
    "section": "",
    "text": "Access tutorial(s) below. For now, only a tutorial in English is available. The tutorial in French is under construction.\nA presentation of some of the equivalent functions to be used in an R console (with a complete tutorial on how to code the statistical analyses) is available here."
  },
  {
    "objectID": "TutorielFR.html#un-rapide-tour-dhorizon-de-mendak",
    "href": "TutorielFR.html#un-rapide-tour-dhorizon-de-mendak",
    "title": "Tutorial (English version)",
    "section": "1 Un rapide tour d’horizon de Mendak",
    "text": "1 Un rapide tour d’horizon de Mendak\nMendak1 est une application R Shiny développée à but pédagogique pour introduire l’analyse statistique textuelle auprès d’étudiant·es qui ne manipulent de langage de programmation informatique (en particulier, R).\nL’application permet de se familiariser aux outils les plus courants de la statistique textuelle à partir d’exemples simples proposés au sein de l’application ou de jeux de données que l’utilisateur·rice charge lui·elle-même dans l’application.\nL’interface de l’application est en anglais, mais les corpus analysables peuvent être en français.\n\nL’application ne présente pas le code R sous-jacent aux différentes opérations conduites ici, mais on pourra en retrouver quelques unes écrites dans des scripts R ici.\nL’application Mendak est composée de trois onglets principaux (contenant des sous-onglets) :\n\nL’onglet “Gestion des Données” : permet d’importer et de formater son jeu de données pour l’analyse. Différents formats d’imports peuvent être utilisés (csv, excel, rdata…), mais les données doivent être organisées dans des tableaux lignes x colonnes, où l’une des colonnes contient le corpus à analyser. Trois jeux de données d’exemples sont également proposés :\n\nLa base des annonces matrimoniales indiennes (en anglais) correspond à un extrait (de 1000 profils) d’une base de données créée grâce au web scraping d’un site internet d’annonces matrimoniales en Inde, qui ont fait l’objet d’une analyse avec Jeanne Subtil dans ce document de travail. Les informations présentées correspondent à certains marqueurs biographiques et familiaux des utilisateurs (résident dans la région de l’Uttar Pradesh) fréquentant le site internet. Deux variables textuelles sont disponibles : la description de sa famille et la description du partenaire idéal.\nLa base des jugements administratifs (en français) correspond à l’ensemble des jugements rendus par des tribunaux administratifs en décembre 2024 qui mentionnent le terme “harcèlement moral”. 142 jugements sont disponibles à l’analyse et on s’est restreint ici aux jugements de première instance (on n’a donc pas gardé les jugements des Cours administratives, ni du Conseil d’État). Cette base correspond à un extrait d’un jeu de données plus large en cours d’analyse par Laurent Willemez.\nLa base des articles de presse autour du “wokisme” correspond à 117 articles de la presse nationale dont le terme “wokisme” est présent dans le titre de l’article. Ces articles ont été extraits à partir des archives Europresse et mis en forme grâce à l’application EuroDecodeur également disponible en ligne, en reprenant une méthode proposée par Corentin Roquebert.\n\nL’onglet de “Statistiques descriptives” permet de réaliser des statistiques univariées et bivariées simples sur les variables quantitatives et qualitatives du jeu de données.\nL’onglet “Analyse Textuelle” est le coeur de Mendak : il permet d’abord de nettoyer son corpus, puis de l’analyser grâce à différents outils. Mendak permet notamment de mobiliser un algorithme de classification des différents textes analysés, en reprenant l’outil rainette proposé par Julien Barnier. La variable de classification peut ensuite être ajoutée au jeu de données pour être analysée dans l’onlet Statistiques Descriptives."
  },
  {
    "objectID": "TutorielFR.html#chargement-et-nettoyage-global-de-la-base-de-données",
    "href": "TutorielFR.html#chargement-et-nettoyage-global-de-la-base-de-données",
    "title": "Tutorial (English version)",
    "section": "2 Chargement et nettoyage global de la base de données",
    "text": "2 Chargement et nettoyage global de la base de données\nDans l’onglet Data Management, la fenêtre “Upload and Download” permet d’importer son jeu de données.\nL’application accepte les fichiers csv, xls, xlsx et RData. Le jeu de données doit avoir une ligne par texte, où les textes sont stockés dans une colonne. Les autres colonnes sont les propriétés des textes (c’est-à-dire les caractéristiques du corpus).\n\nLe fichier d’exemple pour ce tutoriel correspond à celui des jugements des Tribunaux Administratifs. Comme l’indique Mendak, 142 jugements sont contenus dans le fichier csv chargé et la base comprend 7 colonnes. En réalité, le fichier original n’en contient que 6, mais le chargement a automatiquement créé une variable de comptage du nombre de mots (variable nwords_jugement) de la variable “texte” (variable jugement).\nLes autres variables correspondent au tribunal où a été rendu le jugement (Juridiction), au type de jugement (Catégorie, correspondant à un jugement en référé pour une ordonnance), au rejet ou nom de la demande (Rejet, cette variable a été créée automatiquement et peut présentée des erreurs), à la date du jugement (Date_decision), et au numéro de jugement tels qu’indiqué par l’administration (NumDecision).\nDans l’application, par défaut, celle-ci tente de reconnaître le “type” des variables contenues dans le jeu de données. Elles peuvent être de trois types différents :\n\nSi une variable est quantitative (par exemple l’âge en chiffres), elle est alors stockée comme “numeric” (numérique). C’est le format adapté pour toutes les données chiffrées qui peuvent être utilisées dans des calculs mathématiques. Notons ici que le numéro de décision est reconnu comme variable numérique mais que ce n’est pas forcément très judicieux de la laisser ainsi : le calcul d’une moyenne n’aurait par exemple aucun sens sur cette variable.\nSi une variable est qualitative avec un petit nombre fini de catégories (par exemple Rejet avec Requête rejetée ou Requête non rejetée), elle est alors stockée comme “factor” (facteur).\nSi une variable est qualitative avec un grand nombre de valeurs différentes et que chaque élément est long (en nombre de caractères), elle est alors codée comme “character” (caractère). Ce sont ces variables que l’application pourra utiliser pour effectuer des analyses textuelles.\n\nIl est crucial de bien définir le type de toutes les variables lors de l’importation du jeu de données car cela conditionne les analyses que l’on pourra effectuer par la suite.\nCet onglet propose plusieurs possibilités de nettoyage des données :\n\nModifier le nom d’une variable\nModifier le nom des catégories pour les variables qualitatives\nChanger le type de variable (si possible)\nModifier l’ordre des catégories (niveaux) pour les variables de type facteur. Ce peut être particulièrement utile pour effectuer des statistiques descriptives. Pour ce faire, cliquez sur les trois petites lignes horizontales et faites glisser une catégorie vers le haut ou vers le bas.\nModifier les étiquettes des catégories pour les variables de type facteur.\n\nN’oubliez pas de cliquer sur “Appliquer les modifications” après avoir modifié une variable. Le jeu de données peut être consulté dans l’onglet “Visualisation”.\nLes autres options de nettoyage des données (recodage, etc.) ne sont pas disponibles sur cette application (et il n’est pas prévu d’ajouter cette fonctionnalité !) et doivent être effectuées avant d’importer le jeu de données dans l’application."
  },
  {
    "objectID": "TutorielFR.html#quelques-statistiques-descriptives",
    "href": "TutorielFR.html#quelques-statistiques-descriptives",
    "title": "Tutorial (English version)",
    "section": "3 Quelques statistiques descriptives",
    "text": "3 Quelques statistiques descriptives\nLe deuxième onglet principal de Mendak permet d’explorer les variables non textuelles du jeu de données importé en effectuant des statistiques univariées et bivariées. Notez qu’aucune statistique inférentielle n’est disponible ici ; l’objectif est de connaître la structure du jeu de données.\nLe jeu de données comprend par exemple presque deux fois plus de requêtes rejetées qu’acceptées :\n\nEt les requêtes rejetées font l’objet en moyenne d’un jugement moins long que celles qui ne le sont pas :"
  },
  {
    "objectID": "TutorielFR.html#analyser-le-corpus-du-jeu-de-données",
    "href": "TutorielFR.html#analyser-le-corpus-du-jeu-de-données",
    "title": "Tutorial (English version)",
    "section": "4 Analyser le corpus du jeu de données",
    "text": "4 Analyser le corpus du jeu de données\nL’onglet d’analyse le plus intéressant (et probablement le moins évident à comprendre quand on débarque) est celui lié à l’analyse textuelle.\n\n4.1 Le nettoyage des données\nAvant de procéder à une analyse du corpus constitué par les 142 textes des jugements, nous devons “nettoyer” ce corpus. Mendak a ici automatiquement sélectionné la variable à nettoyer comme étant celle du jugement, c’est en effet la seule de type “character” dans ce jeu de données (il faut faire attention dans les cas où il y en a plusieurs !).\nLes premières options proposées sont des options de “nettoyage” assez courantes dérivées du package quanteda et permettent de supprimer un certain nombre d’éléments “polluants” du texte pour se concentrer sur les mots. Je recommande donc de supprimer la ponctuation du texte, les nombres, les symboles (par exemple *$`£), de convertir en minuscules (pour que “Texte” soit équivalent à “texte”) et de supprimer les mots “vides”.\nCes mots vides sont couramment supprimés dans l’analyse de corpus car ils n’ont pas de sens particulier (ceci ou cela par exemple, plus générament des pronoms, articles, prépositions, verbes auxiliaires…) ou ils ont un sens trop ambigu pour être retenus dans l’analyse. Les supprimer permet d’éviter de “polluer” le corpus et d’étudier des associations statistiques qui n’ont en fait pas vraiment de sens. Quanteda propose des banques de mots vides (des “stop words” en anglais) et Gilles Bastin a récemment proposé une liste plus complète de mots vides qui est intégrée à cette application et qu’on recommande d’utiliser pour les corpus en langue française.\nNotons qu’on peut tout à fait ajouter soi-même des “mots vides” dans Mendak, il suffit de cliquer sur “Remove words manually from corpus” et d’ajouter sa propre liste, où les mots sont séparés par des virgules.\nUne option est également disponible permettant d’effectuer un nettoyage de texte plus approfondi, en utilisant la reconnaissance et l’annotation automatique de texte basées sur les banques de mots UDPipe. L’idée est de coder tous les mots des textes en les comparant à un dictionnaire de banque de mots (qui dépend de la langue dans laquelle vous travaillez).\n\nDeux options différentes sont disponibles en cochant cette option :\n\nSélectionner les mots selon leur “forme”. Par exemple, vous pouvez considérer que travailler avec les noms, les adjectifs et les verbes est suffisamment riche pour saisir la variété des différents sujets dans un corpus et que les autres formes de mots risqueraient de perturber l’analyse. Ou alors on peut se dire que justement, travailler sur les verbes peut être tout à fait éclairant.\nLemmatiser le corpus. La lemmatisation consiste à regrouper les différentes formes fléchies d’un mot pour les analyser comme un seul élément. Par exemple, “meilleur” a “bon” comme lemme, “familles” a “famille” comme lemme, etc… Il existe différentes règles de lemmatisation et l’application ne propose ici qu’une version automatisée. Je recommande de ne lemmatiser que dans un second temps, après avoir exploré les analyses sans lemmatisation, pour éviter de regrouper des mots comme “chats” et “chiens”. Par ailleurs, qui “lemmatise dilemne attise” comme l’a écrit Etienne Brunet et cette opération de transformation radicale d’un corpus de textes n’est pas forcément recommandée.\n\nEn général, cette étape de nettoyage utilisant UDPipe prend un certain temps à s’exécuter et il n’est pas forcément recommandé de tenter cette procédure sur des corpus volumineux. Précisons aussi que ces options sont sans doute les plus expérimentales de l’application et je ne garantis pas leur fiabilité (d’autant que dans la pratique la non-lemmatisation apporte déjà des résultats très intéressants).\nAvant de valider le nettoyage du texte, trois paramètres restent à régler :\n\nJ’ai inclus une option pour supprimer les mots courts. En fait, ce sont souvent des “mots vides” traités par la suppression des “mots vides”, mais dans un autre exemple (les annonces matrimoniales), quelques mots courts cachés subsistaient car ils étaient dans une autre langue (en hindi) que celle reconnue par l’application (l’anglais). On peut donc ici abaisser cette limite à 2.\nJ’inclus également une option permettant de sélectionner les mots selon leur nombre minimum d’occurrences dans le corpus. De nombreux mots sont très rares. Il y a toujours des mots qui sont des “hapax” (n’apparaissant qu’une seule fois) et la loi de Zipf prédit généralement que le mot le plus commun apparaît environ deux fois plus souvent que le suivant, trois fois plus souvent que le troisième plus commun, et ainsi de suite. Le seuil retenu ne doit pas être trop bas (cela tendra sinon dans la classification à produire des groupes avec un petit nombre de textes) mais pas trop élevé non plus (si nous ne gardons que les mots communs, alors ils ne sont pas distinctifs entre les textes du corpus). Je laisse ici le paramètre à 10.\nEnfin, la dernière option n’est pas une étape de nettoyage des données en tant que telle, mais plutôt une caractéristique de la manière dont nous subdivisons les textes du copus. En effet, pour effectuer une analyse par classification, nous pouvons vouloir regrouper des paragraphes (appelés segments dans l’application) plutôt que des textes entiers. En outre, pour analyser les cooccurrences de mots (quels sont les mots les plus fréquents qui apparaissent conjointement avec d’autres mots), nous le ferons au niveau du segment plutôt qu’au niveau du texte. Ici, le découpage en segments va peut-être permettre de distinguer dans les jugements des sections liées à la présentation des faits, du jugement en tant que tel, etc. J’ai retenu le découpage en 40 mots tel que proposé par l’application.\n\nLe nuage de mots permet de repérer rapidement les mots retenus qui vont faire l’objet d’une analyse statistique, ici seulement les 100 mots les plus fréquents sont représentés (sur 1457). On voit que le langage juridique a de l’importance… et que le corpus pourrait bénéficier d’un nettoyage un peu plus approfondi (mme pour madame, n’a, n’est, qu’il, etc qui sont autant de mots vides).\n\n\n\n4.2 Un nuage de mots des principales oppositions\nPlutôt que de calculer un nuage de mots sur l’ensemble du corpus, il est possible de calculer les mots les plus distinctifs par groupe (sur la base des variables qualitatives de l’ensemble de données). Les mots distinctifs utilisent une mesure de « keyness » basée sur la statistique du chi2 (et peuvent être trouvés dans l’onglet Tableau).\nIci, les requêtes non rejetées se distinguent avant tout parce qu’elles mentionnent le dédommagement retenu pour le préjudice subi (ou les préjudices !), le harcèlement moral est associé à d’autres préjudices de santé, et certaines requêtes semblent liées à des centres hospitaliers.\n\n\n\n4.3 La stratification des occurrences de mots\nOn peut ici s’interroger si certains mots sont plus spécifiques à certains types de textes de notre corpus. Dans l’onglet précédent, on a vu que la “maladie”, la “santé” était davantage caractéristique des requêtes acceptées. Le harcèlement moral, redoublé de conséquences négatives pour la santé, a-t-il plus de chances d’aboutir ?\nOn peut par exemple observer que les jugements mentionnant la “maladie” sont certes moins nombreux dans le cas où la requête est acceptée, mais c’est aussi parce qu’il y a moins de jugements dont la requête est acceptée, de telle sorte qu’il y a relativement plus de jugements de cette catégorie qui mentionnent ce terme (52% contre 31% quand la requête est rejetée).\nNotons par ailleurs, la mention de ce terme se fait plusieurs fois dans le même jugement, et que cette pluri-occurrence est plus forte dans le cas où la requête est acceptée plutôt que quand elle est rejetée.\n\n\n\n4.4 L’analyse géométrique des données\nCet onglet propose une analyse géométrique des données (AGD) permettant de repérer dans l’ensemble du corpus les associations les plus fortes entre les différents mots et les textes dans lesquels ils se trouvent.\nIl s’agit d’avoir une analyse synthétique de ce jeu de données énorme constitué par le corpus des textes des jugements.\nL’analyse géométrique des données est un bon outil pour cette mise à plat exploratoire puisqu’à partir d’une matrice de comptage de la présence des mots dans chacun des textes, elle permet d’identifier les mots qui sont le plus fréquemment employés avec d’autres dans le même texte et au contraire des mots qui sont rarement employés dans les mêmes textes.\nEn fait, ce que l’AGD dans cet onglet cherche à faire c’est :\n\nRésumer la richesse des informations de notre corpus transformé en base de données en quelques représentations graphiques, où les principaux traits saillants de notre corpus sont résumés par des “axes”, aussi appelés “dimensions” ou encore “facteurs”.\nCes “axes” ou “dimensions” représentent les dimensions de variabilité du corpus : deux mots proches dans la représentation graphique sont plus souvent qu’en moyenne employés dans les mêmes textes, et deux mots éloignés dans la représentation sont moins souvent qu’en moyenne employés dans les mêmes textes. Par ailleurs, deux textes proches dans l’espace factoriel ont une proximité sémantique : ils se caractérisent par des mots communs qui leur sont typiques et sont différents des autres.\nLe premier “axe” ou la première “dimension” va résumer davantage d’information que la deuxième, qui va elle même en résumer moins que la troisième, etc. Chaque information apportée par une dimension est indépendante de l’autre, de sorte qu’on n’a pas de redondance de l’information.\n\nEn d’autres termes, on ne classifie pas encore (ça se passe dans un autre onglet), mais on commence à s’en approcher puisqu’on cherche à relever des proximités et des différences.\nOn propose ici deux variantes de l’AGD :\n\nune Analyse en Composantes Principales (ACP) où le tableau analysé est de la forme individu statistique (ici, les jugements) x variables (ici, les mots du corpus), et on utilise la corrélation de Spearman plutôt que celle de Pearson pour étudier les associations entre mots (en effet, les distributions des mots ne suivent pas une distribution statistique “normale” et la petite variante sur la mesure de la corrélation vise à pallier ce problème). Une telle ACP se nomme aussi “ACP sur rangs” (car on calcule la corrélation des rangs).\nun Analyse Factorielle des Correspondances (AFC) où le tableau analysé est de la forme lignes (jugements) x colonnes (mots du corpus), donc un tableau de contingence. Cette méthode est la plus classiquement utilisée pour en AGD lorsqu’on a affaire à des textes. Cette petite vidéo est une introduction très rapide à l’AFC.\n\nIci, en employant l’AFC on voit que le premier axe horizontal est complètement structuré par la spécificité des ordonnances en référés qui structurent la partie droite sur l’axe 1.\n\n\nLe deuxième graphique est obtenu en ajoutant les caractéristiques des textes : ces variables supplémentaires (et non actives) ne participent pas à la structuration des axes, mais facilitent leur interprétation en repérant des associations entre types de textes et mots associés.\nPar ailleurs, on voit aussi que les axes 2 et 3 semblent apporter une information à peu près équivalente dans le nuage des mots (c’est l’onglet explained variance of each factorial dimension).\nRegardons donc de plus près les axes 2 et 3 associés dans le même plan factoriel (axe 2 à l’horizontal et 3 à la verticale).\n\nCe plan factoriel distingue des requêtes pour harcèlement moral dans trois types de structures : en haut à gauche dans des centres hospitaliers, en haut à droite dans des entreprises qui ont des délégations de service public (on se plaint alors de licenciement) et en bas auprès de communes.\n\n\n4.5 Le contexte des mots\nDe manière plus qualitative, on peut rechercher le contexte des mots dans les phrases. Il suffit de taper un mot (ici, un terme non nettoyé !) et Mendak affichera les différentes phrases dans lesquelles il est utilisé.\nLa recherche du mot “soins” permet de révéler une possible ambiguité de sens : le terme a un double usage, le soin médical, mais aussi l’expression “par ses soins”. Il peut être plus sage dans une version plus avancée d’ajouter ce terme parmi les “mots vides”.\n\n\n\n4.6 La cooccurrence des mots les plus fréquents\nL’étude des cooccurrences dans l’analyse textuelle a été soulignée comme une amélioration clé pour obtenir des informations sur les textes. Nous définissons ici une cooccurrence comme l’association qui relie des mots dans un texte, ces associations se situant au niveau du segment (paragraphe) plutôt que du texte entier.\nTout d’abord, on peut visualiser les 50 mots les plus fréquents (mots nettoyés) dans le corpus et leur niveau de cooccurrence, comme le montre le réseau ci-dessous. La largeur des lignes représente le degré de cooccurrence (une largeur plus importante signifie que les mots cooccurrent plus souvent) et les mots positionnés au centre cooccurrent davantage avec d’autres mots.\nCe réseau de mots permet de repérer les mots juridiques (on s’interroge s’il y a “préjudice”), les conditions associées au harcèlement moral (la santé), etc (attention à jouer sur la taille des labels en dessous du graphique pour faire apparaitre tous les mots, la taille des mots étant proportionnelle à leur fréquence d’occurrence).\n\n\n\n4.7 La cooccurrence d’un mot avec les mots les plus fréquents\nOn peut aussi chercher un mot spéficique et la manière dont dans le même segments de mots, il est associé de manière fréquente à d’autres termes, ici par exemple avec la recherche du mot “agissements”, fortement associé au harcèlement moral, mais aussi à sa définition (il faut qu’il y ait des agissements “répétés” pour que le harcèlement soit caractérisé, ou que ce soit “constitutif” de harcèlement comme l’écrivent les juges).\n\n\n\n4.8 La classification des textes\nL’une des principales caractéristiques de l’application est d’inclure des algorithmes de classification pour regrouper les textes et identifier les différents sujets/univers lexicaux.\nDétails sur le fonctionnement des algorithmes (basés sur une classification hiérarchique descendante) :\n\nJulien Barnier fournit une description de l’usage de cet algorithme de classification et une description approfondie complète des algorithmes (nous utilisons exactement les mêmes fonctions du paquet rainette).\nVous pouvez également regarder son tutoriel au séminaire Mate-shs.\n\nSi les textes analysés sont assez courts, je recommande d’effectuer la classification sur les “documents” (c’est-à-dire les différents textes, une ligne par texte dans le jeu de données initial).\nOn retient ici une classification des jugements en quatre groupes qui semblent faire ressortir différents motifs de requêtes ou types de procédure. Le premier onglet des résultats fait apparaitre un dendrogramme comme ci-dessous, c’est à dire un “arbre de classification”, qui montre plusieurs choses :\n\nles “branches” permettent de rendre compte de la plus ou moins forte proximité des classes de jugements découpés (ainsi le cluster 1 est le plus différent des autres clusters, et les clusters 3 et 4 sont les clusters les plus proches)\nla taille des clusters, où le cluster 1 rassemble 25 jugements, soit 18% des jugements analysés\nles mots les plus “typiques” associés à chacun des clusters (en bleu) et éventuellement ceux qui sont les moins associés (en rouge)\n\nCette dernière information est cruciale pour interpréter les différents types de jugements, qu’on peut également interpréter en retournant au texte (c’est l’onglet Documents/Segments by class) :\n\nLe cluster 1 correspond à des ordonnances en référés qui sont pris en cas d’urgence. Ces jugements ne jugent pas sur le fond d’une affaire, mais demandent une “suspension” (d’une décision administrative). De manière intéressante, l’université semble être une administration où ces demandes sont assez fréquentes…\nLe cluster 2 correspond à des requêtes où les requérants avaient sollicité une “protection fonctionnelle” auprès de leur hiérarchie (pour se protéger de faits de harcèlement moral) et qui leur a été refusée.\nLe cluster 3 correspond à des affaires où les requérants contestent leur licenciement.\nLe cluster 4 se distingue parce que les demandes des requérants sont avant tout des demandes de réparation en termes financiers (au vu d’un ou de plusieurs préjudices subis).\n\n\n\n\n4.9 La classification des segments des textes\nL’autre type de classification consiste à classer non pas les textes (les différents jugements), mais les “segments” ou paragraphes de ces jugements. Cette méthode de classification est plus couteuse et prend un peu plus de temps à tourner…\nOn peut commencer par réaliser une classification “simple”, pour décider du nombre de classes le plus judicieux à retenir, avant de renforcer la robustesse de la classification grâce à une classification “duale”.\nL’un des intérêts de ce type de classification quand on a ici affaire à des jugements, c’est que ces textes sont assez formatés, avec une partie qui rappelle les requêtes des requérants, une partie qui expose l’argumentaire juridique, une partie sur les décisions du tribunal.\n\n\n4.10 L’ajout des variables de classification à la base de données\nAprès avoir classé les textes, il est possible d’ajouter ces informations à l’ensemble de données initial :\n\nSi l’algorithme de classification a été effectué sur des textes/documents, chaque ligne de l’ensemble de données peut se voir attribuer un numéro de cluster stocké dans une variable unique (choisissez le nom de la variable avant d’ajouter la variable cluster dans l’ensemble de données).\nSi l’algorithme de classification a été effectué sur des segments, chaque ligne de l’ensemble de données appartient potentiellement en partie à différents clusters, car chaque ligne = un texte divisé en plusieurs segments. Dans ce cas, l’application crée différentes variables (autant de variables que le nombre de clusters dans la partition) et pour chaque colonne calcule la proportion (en %) des segments du texte qui appartiennent à ce cluster. Par exemple, si pour le texte 1, 3 segments sur 10 sont classés dans le cluster 1, alors la colonne cluster 1 indiquera 30 pour cette ligne.\n\nCes variables peuvent ensuite être analysées dans l’onglet Statistiques descriptives.\nPar exemple, par rapport à notre classification des documents, on voit ici qu’alors que les requêtes motivées notamment par un licenciement, elles représentent 41% des requêtes non rejetées, autrement dit, le juge administratif semble être venu réparer une faute administrative."
  },
  {
    "objectID": "TutorielFR.html#footnotes",
    "href": "TutorielFR.html#footnotes",
    "title": "Tutorial (English version)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL’application est disponible en ligne à ces deux adresses : https://shiny.ens-paris-saclay.fr/app/mendak et https://analytics.huma-num.fr/Mathieu.Ferry/mendak/. Normalement les deux versions sont à jour mais on privilégiera la première pour un peu plus de puissance de calcul et la seconde si on veut être sur d’avoir la version la plus à jour.↩︎"
  },
  {
    "objectID": "TutorialEN.html#a-quick-tutorial-to-use-mendak",
    "href": "TutorialEN.html#a-quick-tutorial-to-use-mendak",
    "title": "Tutorial (English version)",
    "section": "1 A quick tutorial to use Mendak",
    "text": "1 A quick tutorial to use Mendak\nMendak is a shiny app designed to help non-coders to conduct relatively easily data and textual statistical analysis.\n\n\n\n\n\nThe app presents more or less the same statistical tools that are presented using R code here. The interested user can thus also delve into these tutorials to reproduce the analyses directly using the R console.\nMendak app is composed of different tabs:\n\nData Management tab: Upload and format your dataset (you can upload it in different formats but it has to be a row x column dataset, where one of the columns contains your text to be analysed). If you do not have a ready-made dataset, use the sample dataset. This dataset is an excerpt (of 1,000 profiles) of a database web scraped from an Indian online matrimonial website, that we analysed in this paper. The information present on each row correspond to the publicly available information of a user describing oneself, their family, their desired partner. Users all reside in Uttar Pradesh, India.\nDescriptive Statistics tab: Run univariate and bivariate statistics on the quantitative and qualitative variables of the dataset.\nTextual Analysis tab: Clean and analyse the text corpus contained in one of the columns of the dataset. Different analyses can be conducted and in case a cluster analysis is conducted, new variable(s) can be added to the dataset to be analysed in the Descriptive Statistics tab."
  },
  {
    "objectID": "TutorialEN.html#upload-and-clean-the-data",
    "href": "TutorialEN.html#upload-and-clean-the-data",
    "title": "Tutorial (English version)",
    "section": "2 Upload and clean the data",
    "text": "2 Upload and clean the data\nIn the Data Management tab (“Upload and download”), start by uploading your dataset. Note that the app accepts csv, xls, xlsx, RData files. The dataset should have one row per text, where the texts are stored in a column. Other columns are properties of the texts (i.e. the corpus characteristics).\n\n\n\n\n\nThe sample dataset is in csv format and can be uploaded by clicking on “Load Sample Data.” Here the dataset contains two different text columns (Desc_family: where each matrimonial profile has described their family background and Desc_desired where users mention what sort of partner they are aspiring to meet).\nBy default, the app tries to recognize the “type” of the variables contained in the dataset. They can be of three different types:\n\nIf a variable is quantitative (e.g. the age in numbers) then it is stored as “numeric”\nIf a variable is qualitative with a small finite number of categories (e.g. sex with male or female categories) then it is stored as “factor” (note that a factor variable can only have a maximum of 10 different unique categories-or levels, otherwise it will be stored as “character” by the app).\nIf a variable is qualitative with a large number of different values and each item is long (in the number of characters), then it is coded as “character.” These variables are the ones the app will be able to conduct textual analysis on.\n\nIt is important to set up the variable type of all variables when uploading the dataset because it conditions the analyses one can do.\nThis tab offers a few data cleaning possibilities:\n\nChange name of variable\nChange name of categories for qualitative variables\nChange variable type (if possible)\nChange the order of categories (levels) for factor variables. It is useful to run descriptive statistics, for instance to re-order the Education levels in a logical way. To achieve this, click on the three small horizontal lines and drag a category up or down\nChange the labels of categories for factor variables\n\nDo not forget to click on “Apply Changes” after modifying a variable. The dataset can be inspected in the “View” tab.\nOther data cleaning options (recoding, etc) are not available on this app and should be done prior to uploading the dataset on the app."
  },
  {
    "objectID": "TutorialEN.html#descriptive-statistics",
    "href": "TutorialEN.html#descriptive-statistics",
    "title": "Tutorial (English version)",
    "section": "3 Descriptive statistics",
    "text": "3 Descriptive statistics\nThe second main tab of Mendak allows to explore non-textual variables of the uploaded dataset by running univariate and bivariate statistics. Note that no inferential statistics are available here; the purpose is to know the structure of the dataset.\nAll cases of data types are taken into account and for factor variables, it is possible to compute both frequency counts (N) or proportions (%)."
  },
  {
    "objectID": "TutorialEN.html#textual-analysis",
    "href": "TutorialEN.html#textual-analysis",
    "title": "Tutorial (English version)",
    "section": "4 Textual Analysis",
    "text": "4 Textual Analysis\nThe most interesting (and less straightforward) tab is the one to run textual analysis.\n\n4.1 Text cleaning\nFirst, one has to clean the texts to run statistical analyses. Mendak allows you to select the variable to clean among variables of type “character.”\nThe first options are rather common “cleaning” options derived from the quanteda package and allow you to delete a number of “polluting” features of text to concentrate on words. So I recommend removing the punctuation from the text, the numbers, symbols (e.g. *$`£), to convert to lowercase (so that Text is equivalent to text) and to remove stopwords (based on a quanteda word bank, including pronouns, articles, prepositions, auxillary verbs…). This last option is available in English and a few other languages.\nYou can also remove words manually from the texts, for instance if you see that a word is highly frequent but does not really bring anything to the analysis (for instance, for the “Desc_family” variable we are interested in how users describe their family, and we know that “family” is going to be a very frequent word, just like parent, father, mother, etc).\n\n\n\n\n\nAn option is also available whereby you can conduct more thorough text cleaning, using automatic text recognition and annotation based on the UDPipe word banks. The idea is to code all the words of the texts by comparing them to a word bank dictionary (which depends on the language you are working with).\n\n\n\n\n\nThen two different options are available:\n\nSelect words depending on their “form.” For instance, you may consider that working with nouns, adjectives and verbs is rich enough to grasp the variety of the different topics in a corpus and that other word forms will confuse the analysis.\nLemmatize the corpus. Lemmatizing means grouping together the different inflected forms of a word to be analysed as a single item, for instance “better” has “good” as its lemma, “families” has “family” as its lemma, etc… There are different rules to lemmatize and the app only suggests an automated version here. I recommend lemmatizing only in a second step after exploring the analyses without lemmatizing to avoid collapsing cats and dogs together.\n\nIn general, this cleaning step using UDPipe takes a bit of time to run and one may well not attempt this procedure on large corpora.\nI included an option to remove short words. In fact, they are often “stopwords” dealt by the above option but in the example used here, there are a few hidden hindi short words (in latin alphabet) that are included in the corpus which I try to get rid of in this way.\nI also include an option to select words according to their minimum of occurrences in the corpus. Many words are very rare. There are always words that are “hapax” (occurring only once) and the Zipf’s law usually predicts that the most common word occurs approximately twice as often as the next common one, three times as often as the third most common, and so on. The threshold should not be too low (it will tend in the clustering to produce clusters with a few number of texts) but not too high either (if we keep only common words then they are not distinctive between the texts of the corpus).\nFinally, the last option is not a data cleaning step as such but rather a feature of how we split the texts. Indeed, to conduct cluster analysis, we may want to cluster paragraphs (called segments in the app) rather than entire texts. Besides, to analyse co-occurrences of words (what are the most frequent words that appear conjointly with other words) we will to this at the segment level rather than at the text level. Here the texts (family descriptions) are rather small (38 words on average) so I split the texts in segments of about 10 words (the function tries to recognize line breaks, commas and dots to cut the segments appropriately).\nAfter clicking on “Clean Text,”, some summary statistics appear, another tab presents the frequency distribution of cleaned words (named features or tokens) and a third tab shows a pretty word cloud of the features within the corpus. Depending on the options you select, different words are kept in the analysis.\n\n\n\n\n\n\n\n4.2 Dual word cloud\nRather than computing a word cloud on the entire corpus, it is possible to compute the most distinctive words by group (based on the qualitative variables of the dataset). The distinctive words use a “keyness” measure based on the chi2 statistic (and can be found in the Table tab).\n\n\n\n\n\n\n\n4.3 Stratified occurrences\nMendak also allows to search for specific words and calculate the proportion (%) of occurrences (i.e. the number of times a word is mentioned over the total number of cleaned occurrences) or documents (i.e. the number of documents mentioning it over the total number of documents) of the words by groups. The groups correspond to the qualitative or quantitative (categorized using quartiles) variables of the dataset.\nFor instance, here, “homemaker” (a synonym implying different a perspective from “housewife”) is slightly more often used by female profiles (but is it the reverse for “housewife”?). The list of searchable words correspond to cleaned words.\n\n\n\n4.4 Context\nIn a more qualitative fashion, one can search for the context of words in sentences. Just type a word (here, a not cleaned term!) and Mendak will show the different sentences in which it is used.\n\n\n\n4.5 Co-occurrences between most frequent words\nThe study of co-occurrences in textual analysis has been underlined as a key improvement for getting insights into texts. Here, we define a co-occurrence is defined as the association that connect words within a text, these associations being at the segment (paragraph) level rather than in the entire text. Two words co-occur if they are used in the same segment of a text.\nFirst, one can visualize the top 50 most frequent words (cleaned words) in the corpus and their level of co-occurrence between them as presented on the network below. The width of lines represents the degree of co-occurrence (a larger width means that words co-occur more often) and words centrally positioned co-occur more with other words.\nHere, words qualifying family members are the most central in the graph. Notice how family is both densely connected to members and to middle and class (themselves connected) and to values. Does it suggest the three patterns by which users present their family (members, socioeconomic, values)?\n\n\n\n\n\n\n\n4.6 Co-occurrences of a specific word\nOne can also search for a specific word and their most frequent co-occurring words. Here, I searched for “settled” because I want to know whether it is used as an adjective (“well-settled”) or a verb designating the location of living. The result suggests that it is both.\n\n\n\n4.7 Document classification\nOne of the key feature of the app is to include classification algorithms to cluster texts and identify different topics/lexical worlds.\nDetails on how the algorithms (based on divisive hierarchical clustering) work:\n\nThe basic one is briefly presented here in English, more details can be found in the English documentation of Iramuteq (see p. 11).\nJulien Barnier also provides a thorough description of the algorithms (we use the exact same functions from the rainette package)\nIn French, Julien Barnier’s description is probably the best. You can also watch his tutorial at the Mate-shs seminar.\n\nDifferent classification algorithms can be performed. If the analysed texts are rather short, I recommend performing the classification on the “documents” (i.e. the different texts, one row per text in the initial dataset).\nHere, I classified the texts into three different clusters. The first one includes 193 texts (almost 20% of the texts), the second one corresponds to 520 clusters (more than half) and the third one includes 281 texts (28% of the texts). Note that 6 texts could not be attributed any class (because after cleaning these texts, they do not contain enough words to be associated to other texts).\nThe first cluster refers to the description of the family as a single unity, described by its values, its socioeconomic status and its living arrangement. The second cluster refers to family descriptions based on individuals’ (members’) occupational position (at the top of the professional occupational scale). The third cluster also refers to members’ occupational or economic activity status, but usually described in less advantageous positions or independent ones (business, farming).\nThis kind of figure is called a “dendrogram” and it shows how close and how distinct clusters are from each other, e.g. cluster 2 and 3 present a higher similarity compared to cluster 1.\nOn the app, it is also possibly to read the different descriptions clustered by the different classes and to search words within them.\n\n\n\n\n\n\n\n4.8 Segment classification\nWe can also try the segment classification (dual one) even if our texts are rather short here. Interestingly, the clusters are rather similar to the ones observed above except that cluster 2 and 3 are inverted.\n\n\n\n\n\nIramuteq users usually perform Correspondance Analysis after running their classification algorithm. This is particularly helpful when running Dual Descending Hierarchical Classification as no dendrogram is visible to observe how similar or distinct clusters are from each other.\nA Correspondance Analysis is run on the matrix ‘clusters x features’, where we keep only the most positively-associated features of each cluster (using the chi2 measure). This analysis is similar to the one that can be obtained from Iramuteq. We attribute words to one cluster (and color them accordingly) based on how the highest positive association in a given cluster.\nThe first dimension (horizontal axis) always depicts the most salient distinctions between clusters while the second axis depicts more secondary distinctions. Clearly the factorial plan shows us that cluster 2 and 3 are relatively more similar to each other and that cluster 1 is very distinct from the other 2.\n\nNotice how some Hindi words pop up in these different analyses, because I haven’t used the UDPipe functions when cleaning the texts.\nHow many groups or clusters of documents should we keep in the final partition?\nThis is an exploratory tool so the best way is to first analyse the two groups and interpret how they are different from each other, then the three groups, etc… At each step, we analyse what distinction emerges with the new cluster.\nAs a rule of thumb:\n\nKeep as many groups as you can interpret them\nRemember that your goal is to be able able to interpret distinctions in your corpus so it is not ideal to have too many clusters! Personally, I find that beyond about 8 clusters, classifications are not informative as it is hard to grasp at a glance the full distinctions.\n\n\n\n4.9 Adding and analysing classes w.r.t. other variables\nAfter classifying the texts, it is possible to add this information to the initial dataset:\n\nIf the classification algorithm was performed on texts/documents, then each row of the dataset can be attributed a cluster number stored in a unique variable (choose the variable name before adding the cluster variable in the dataset).\nIf the classification algorithm was performed on segments, then each row of the dataset potentially partially belongs to different clusters as each row = one text divided in several segments. In that case, the app creates different variables (as many variables as the number of clusters in the partition) and for each column calculates the proportion (in %) of the segments of the text that belong to this cluster. For instance, if for text 1, 3 out 10 segments are classified in cluster 1, then the cluster 1 column will put 30 for this row.\n\nThese variables can then be analysed in the Descriptive statistics tab.\nFor instance, the document classification variable (called “Classif”) analysed along with the family variable (joint or nuclear family of the profile user) shows that cluster 1 (describing one’s family as a unit described by its values, socioeconomic position and living arrangement) is more typical of users coming from joint families.\n\n\n\n\n\nA similar trend can be seen when using the variables from the segment classification, as on average a third of the text descriptions of users coming from joint families belong to the first cluster versus less than a fifth of text descriptions of users belonging to nuclear families."
  }
]